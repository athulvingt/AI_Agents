import os
import uuid
from dotenv import load_dotenv
from operator import itemgetter
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain_community.storage import RedisStore
from langchain_community.utilities.redis import get_client
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser

from langchain_core.runnables import RunnableLambda, RunnablePassthrough

import preprocessor
import utils

#load environment variable
load_dotenv()

#set directory to upload files
UPLOAD_DIR = "uploads"
FIGURES_DIR = 'figures'
CHROMA_DIR =  "./rag_chroma_db"
redis_url = 'redis://localhost:6379'
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(FIGURES_DIR, exist_ok=True)


embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')
chatgpt = ChatOpenAI(model="gpt-4o-mini", temperature=0)
redis_client = get_client(redis_url)
redis_store = RedisStore(client=redis_client)

def save_uploaded_file(file_storage) :
    filename = f"{file_storage.filename}"
    path = os.path.join(UPLOAD_DIR, filename)
    file_storage.save(path)
    return path

def get_chroma_collection(collection_name):
    return Chroma(
        collection_name=collection_name,
        embedding_function=embedding_model,
        collection_metadata={"hnsw:space": "cosine"},
        persist_directory=os.path.join(CHROMA_DIR, collection_name)
    )

def get_retriever(collection):
    vectorstore = get_chroma_collection(collection)
    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore= redis_store,
        id_key = 'doc_id'
    )
    return retriever

def store_document_chunks(retriever, doc_summaries, doc_contents):
    doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
    summary_docs = [
        Document(page_content=s, metadata={'doc_id': doc_ids[i]})
        for i, s in enumerate(doc_summaries)
    ]
    retriever.vectorstore.add_documents(summary_docs)
    retriever.docstore.mset(list(zip(doc_ids, doc_contents)))


def add_document(file_path, collection):
    chroma_path = os.path.join(CHROMA_DIR, collection)
    os.makedirs(chroma_path, exist_ok=True)
    img_path = os.path.join(FIGURES_DIR)
    os.makedirs(img_path, exist_ok=True)
    docs, tables = preprocessor.chunk_file(file_path, img_path)
    text_summaries, text_docs,  table_summaries, table_docs =  preprocessor.generate_text_table_summaries(docs,tables,chatgpt)
    imgs_base64, image_summaries = preprocessor.generate_img_summaries(img_path,chatgpt)
    retriever  = get_retriever(collection)
    # Add texts, tables, and images
    # Check that text_summaries is not empty before adding
    if text_summaries:
        store_document_chunks(retriever, text_summaries, docs)
    # Check that table_summaries is not empty before adding
    if table_summaries:
        store_document_chunks(retriever, table_summaries, table_docs)
    # Check that image_summaries is not empty before adding
    if image_summaries:
        store_document_chunks(retriever, image_summaries, imgs_base64)
    return True

def answer_query(query, collection):
    retriever = get_retriever(collection)
    # Create RAG chain
    multimodal_rag = (
            {
                "context": itemgetter('context'),
                "question": itemgetter('input'),
            }
            | RunnableLambda(preprocessor.multimodal_prompt_function)
            | chatgpt
            | StrOutputParser()
    )

    # Pass input query to retriever and get context document elements
    retrieve_docs = (itemgetter('input')
                     | retriever
                     | RunnableLambda(utils.split_image_text_types))

    # Below, we chain `.assign` calls. This takes a dict and successively
    # adds keys-- "context" and "answer"-- where the value for each key
    # is determined by a Runnable (function or chain executing at runtime).
    # This helps in also having the retrieved context along with the answer generated by GPT-4o
    multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)
                                .assign(answer=multimodal_rag)
                                )

    return multimodal_rag_w_sources.invoke({'input': query})

if __name__ == "__main__":
    import glob
    # from schema import Citation
    # pdf_files = glob.glob('/home/athul/workspace/AI_Agents/data/*.pdf')
    # for fp in pdf_files:
    #     add_document(fp,'test_collection')
    query = "Tell me detailed statistics of the top 5 years with largest wildfire acres burned"
    answer = answer_query(query,'test_collection')
    # print(answer.keys())
